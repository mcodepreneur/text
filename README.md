How would one use recently developed transformer language models small enough to run quickly on small mobile computers to obtain intelligent, context informed typo correction? The machine learning (I assume they use some kind of n-gram model) assisted typing interface on my phone displays suggestions for words it thinks you are going to type, and the suggestions get better as you type the word, but doesn't autocorrect unless the word is misspelled. Even when the user inputs a word that is clearly out of context, the "autocorrector" doesn't intervene. My proposal is to use GPT-2 and BERT to identify when the probability of a word (GPT-2 for last word predictions, BERT for masked, bidirectional, follow up predictions) is low and there is a similar word that maximizes a combined metric of their likelyhood and similarity defined by multiplying the token prediction probabilities, taken to a power 1>p>0: 1.6, by their similarities, only where the SequenceMatching similarity ([0, 1]) between the predicted word and the target word is above or equal to 0.5, then applying a logarithmic transformation to the similarites, mapping it back to [0, 1]: (log(x/0.5)/log(2))**4, [figure] and defining a threshold of this new combined metric to decide whether to correct the target word. The target word would not be corrected to the top predicted correction, even when the the metric for that prediction passes the threshold, if the target word itself is within a set of predictions with scores above 5 percent of the top prediction.
