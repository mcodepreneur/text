{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ef333b-0776-4e7a-9b34-77dc9a9286d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\bills-fish-\n",
      "[nltk_data]     shack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, AutoTokenizer, BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "from difflib import SequenceMatcher\n",
    "from spellchecker import SpellChecker\n",
    "from string import punctuation\n",
    "from time import time\n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298822b9-d219-4630-83ce-e1e42d7dac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "topk = 2000  # number of top predicted tokens to retrieve (before excluding non-words) \n",
    "\n",
    "class GPT2:\n",
    "    def __init__(self, model=\"gpt2\"):\n",
    "        self.model     =   GPT2LMHeadModel.from_pretrained(model)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model)\n",
    "        self.model_id  = model\n",
    "    \n",
    "    def get_word_probs(self, sentence, n=topk):  # adapted from raul on stackoverflow\n",
    "        inputs = self.tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs[0]\n",
    "        candidates = predictions[0, -1, :]                          # Get the next token candidates.\n",
    "        topk_i = torch.topk(candidates, n).indices.tolist()         # Get the top k next token candidates.\n",
    "        all_probs = torch.nn.functional.softmax(candidates, dim=-1) # Get the token probabilities for all candidates.\n",
    "        topk_probs = all_probs[topk_i].tolist()                     # Filter the token probabilities for the top k candidates.\n",
    "        topk_tokens = [self.tokenizer.decode([idx]).strip()         # Decode the top k candidates back to words.\n",
    "                       for idx in topk_i]\n",
    "        return list(zip(topk_tokens, topk_probs))\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self, model=\"google-bert/bert-base-uncased\"):\n",
    "        self.model     = BertForMaskedLM.from_pretrained(model)\n",
    "        self.tokenizer =   AutoTokenizer.from_pretrained(model)\n",
    "        self.model_id  = model\n",
    "        \n",
    "    def get_word_probs(self, prompt, topk=topk):                  # Get topk masked token candidates\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        mask_index  = (inputs.input_ids == self.tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        mask_logits = logits.squeeze()[mask_index].squeeze()\n",
    "        probs = softmax(mask_logits, dim=-1)\n",
    "        topk = 5000\n",
    "        topk_probs, topk_i = torch.topk(probs, topk, dim=-1)\n",
    "        topk_tokens = np.array([self.tokenizer.decode([i]) for i in topk_i])\n",
    "        return np.hstack((topk_tokens.reshape(-1,1), np.array(topk_probs).reshape(-1,1)))\n",
    "\n",
    "M_GPT2 = GPT2(\"gpt2\")\n",
    "M_BERT = BERT(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28778b24-f3cf-4081-ae8b-234040de1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    common_len = round((len(a)+len(b))/2)\n",
    "    adjustment = 0\n",
    "    adjustment_table = {1: 0.4, 2: 0.3, 3: 0.2, 4: 0.1}\n",
    "    if common_len in adjustment_table: adjustment = adjustment_table[common_len]*(np.e**(-1*np.abs(len(a)-len(b))))\n",
    "    return SequenceMatcher(None, a, b).ratio() + adjustment\n",
    "def rreplace(string, word, new_word):\n",
    "    start = string.rfind(word)\n",
    "    return string[0:start] + new_word + string[start+len(word):]\n",
    "wl          = set(nltk.corpus.words.words())\n",
    "log_map     = lambda e: np.vectorize(lambda x: np.power(np.log(x/0.5)/np.log(2), e))  # specify exponent to return vectorized mapping\n",
    "after_slash = lambda x: x[(x.rfind(\"/\")+1 if x.rfind(\"/\") != -1 else 0):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da60ee3e-f0a4-489f-8318-636abf0fd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(string, back_n):\n",
    "    places = range(1,back_n+1)\n",
    "    words  = string.split()\n",
    "    for n in places:\n",
    "        if n > len(words): break\n",
    "        if n > 1:\n",
    "            model  = M_BERT\n",
    "            masked = \"[MASK]\" + words[-n][-1] if not words[-n][-1].isalpha() else \"[MASK]\"\n",
    "            target = words[-n].strip(punctuation)\n",
    "            prompt = ' '.join(words[:-n] + [masked] + words[len(words)-(n-1):])\n",
    "        else:\n",
    "            model  = M_GPT2\n",
    "            string = string.strip()\n",
    "            last_space = string.rfind(' ')\n",
    "            prompt = string[:last_space]\n",
    "            target = string[last_space+1:].strip(punctuation)\n",
    "        probs  = model.get_word_probs(prompt)                \n",
    "        probsp = [(str(word), float(prob), float(similar(target, word))) for word, prob in probs if word in wl]\n",
    "        close_probs = [prob for prob in probsp if prob[2] > 0.5 and prob[1] >= min(0.001, probsp[consider_top][1])]\n",
    "        props = [(word, (prob**prob_exp)*log_map(log_exp)(sim)) for word, prob, sim in close_probs]\n",
    "        props = sorted(props, reverse=True, key=lambda x: x[1])\n",
    "        props = [prop for prop in props if prop[1] > 0.000001]\n",
    "        probN = threshold(n)\n",
    "        make_correction = False\n",
    "        if len(props) > 0 and props[0][1] > probN:\n",
    "            make_correction = True\n",
    "            irr_t = props[0][1] * relevency_t\n",
    "            for word, score in props: \n",
    "                if score < irr_t: break\n",
    "                elif target.lower() == word.lower():\n",
    "                    make_correction = False\n",
    "        if make_correction: return (n, props[0][0])\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3f5aa2-3d7c-42b5-be74-e297bf89b78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 'feed')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = [#\"when you come in can you remember to feel the cat\",\n",
    "           #\"I have to right a note\",\n",
    "           #\"This is Eric. He's going to fry to\",\n",
    "           #\"do you want any walt or pepper\",    \n",
    "           #\"Please don't forget to turn off the store when\",\n",
    "           #\"are you going to wear the yellow hat or the bed one\",\n",
    "           #\"when do you want to get up to see the fun rise?\",\n",
    "           #\"can you go and let my\",\n",
    "           #\"can you let my\",\n",
    "           #\"you led us on a wild goose case\",\n",
    "           #\"when you come over can you remember to being\",\n",
    "           #\"when you come over can you being the soup\",\n",
    "           #\"when you come over can you, being the\",\n",
    "           #\"I went outside and the wind flew my hat\",\n",
    "           #\"After I get out of the shower I usually growl\",  # don't correct\n",
    "           #\"Don't step on my wet bug\", \n",
    "           #\"the rally\",\n",
    "           #\"can you really climb all the way up that really tall birdling?\",\n",
    "           #\"I think you need to pat more attention\",\n",
    "           #\"This method isn't really as grate\",\n",
    "           #\"Don't step on the wet floor, we're freaching\",  # don't correct\n",
    "           #\"Who dat\",  # don't correct\n",
    "           #\"When you come over, can you bring the flock we talked about?\"\n",
    "           ]\n",
    "back_n = 3  # number of words back from end of string, 1 is just last word\n",
    "\n",
    "log_exp        = 4  # exponent parameter for logarithmic mapping\n",
    "prob_exp       = 1.6  # raise probability to power in ((prob**power)*log-sim)\n",
    "consider_top   = 200  # max top model word predictions considered\n",
    "relevency_t    = 0.2  # threshold defined by portion of top proposition to exclude much smaller scored propositions for correcting\n",
    "base_t         = 0.0001  # decision threshold for last word: base threshold\n",
    "threshold_type = \"jump-exp\"  # function defines decision threshold for word n from end\n",
    "threshold      = {\"constant\":    lambda n: base_t,\n",
    "                  \"linear\":      lambda n: base_t + (base_t * (n-1)),\n",
    "                  \"exponential\": lambda n: base_t * (n**2),\n",
    "                  \"jump-exp\":    lambda n: base_t * (max(n-1,1)**2),        # jump thresholds start growing after n=2\n",
    "                  \"jump-lin\":    lambda n: base_t + (base_t * max(n-2, 0))\n",
    "                 }[threshold_type]\n",
    "\n",
    "correct(\"when you come in can you remember to feel the cat\", 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
