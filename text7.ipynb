{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ef333b-0776-4e7a-9b34-77dc9a9286d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\bills-fish-\n",
      "[nltk_data]     shack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from transformers import pipeline, GPT2TokenizerFast, GPT2LMHeadModel, AutoTokenizer, BertForMaskedLM\n",
    "from autocorrect import Speller\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from difflib import SequenceMatcher\n",
    "from string import punctuation\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298822b9-d219-4630-83ce-e1e42d7dac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "topk = 2000  # number of top predicted tokens to retrieve (before excluding non-words) \n",
    "\n",
    "class GPT2:\n",
    "    def __init__(self, model=\"gpt2\"):\n",
    "        self.model     =   GPT2LMHeadModel.from_pretrained(model)\n",
    "        self.tokenizer = GPT2TokenizerFast.from_pretrained(model)\n",
    "        self.model_id  = model\n",
    "    \n",
    "    def get_word_probs(self, sentence, n=topk):  # adapted from raul on stackoverflow\n",
    "        inputs = self.tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            predictions = outputs[0]\n",
    "        candidates = predictions[0, -1, :]                          # Get the next token candidates.\n",
    "        topk_i = torch.topk(candidates, n).indices.tolist()         # Get the top k next token candidates.\n",
    "        all_probs = torch.nn.functional.softmax(candidates, dim=-1) # Get the token probabilities for all candidates.\n",
    "        topk_probs = all_probs[topk_i].tolist()                     # Filter the token probabilities for the top k candidates.\n",
    "        topk_tokens = [self.tokenizer.decode([idx]).strip()         # Decode the top k candidates back to words.\n",
    "                       for idx in topk_i]\n",
    "        return np.array(list(zip(topk_tokens, topk_probs)))\n",
    "\n",
    "class BERT:\n",
    "    def __init__(self, model=\"google-bert/bert-base-uncased\"):\n",
    "        self.model     = BertForMaskedLM.from_pretrained(model)\n",
    "        self.tokenizer =   AutoTokenizer.from_pretrained(model)\n",
    "        self.model_id  = model\n",
    "        \n",
    "    def get_word_probs(self, prompt, topk=topk):                  # Get topk masked token candidates\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        mask_index  = (inputs.input_ids == self.tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        mask_logits = logits.squeeze()[mask_index].squeeze()\n",
    "        probs = softmax(mask_logits, dim=-1)\n",
    "        topk = 5000\n",
    "        topk_probs, topk_i = torch.topk(probs, topk, dim=-1)\n",
    "        topk_tokens = np.array([self.tokenizer.decode([i]) for i in topk_i])\n",
    "        return np.hstack((topk_tokens.reshape(-1,1), np.array(topk_probs).reshape(-1,1)))\n",
    "\n",
    "M_GPT2 = GPT2(\"gpt2\")\n",
    "M_BERT = BERT(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28778b24-f3cf-4081-ae8b-234040de1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    common_len = np.ceil((len(a)+len(b))/2)\n",
    "    adjustment = 0\n",
    "    adjustment_table = {1: 0.5, 2: 0.3, 3: 0.2, 4: 0.1}\n",
    "    if common_len in adjustment_table: adjustment = adjustment_table[common_len]*(np.e**(-k*(np.abs(len(a)-len(b))-ap))-bp)\n",
    "    return SequenceMatcher(None, a, b).ratio() + adjustment\n",
    "def rreplace(string, word, new_word):\n",
    "    start = string.rfind(word)\n",
    "    return string[0:start] + new_word + string[start+len(word):]\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "lemma       = lambda x: lemmatizer.lemmatize(x)\n",
    "stemmer     = PorterStemmer()\n",
    "stem        = lambda x: stemmer.stem(x)\n",
    "spell       = Speller()\n",
    "wl          = set(nltk.corpus.words.words())\n",
    "log_map     = lambda e: np.vectorize(lambda x: np.power(np.log(x/0.5)/np.log(2), e))  # specify exponent to return vectorized mapping\n",
    "after_slash = lambda x: x[(x.rfind(\"/\")+1 if x.rfind(\"/\") != -1 else 0):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da60ee3e-f0a4-489f-8318-636abf0fd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(string, back_n):\n",
    "    probs = []\n",
    "    places = range(1,back_n+1)\n",
    "    string = string.strip()\n",
    "    words  = string.split()\n",
    "    last_space = string.rfind(' ')\n",
    "    for n in places:\n",
    "        if n > len(words) or len(words) == 1: break\n",
    "        spelled = False\n",
    "        if n > 1:\n",
    "            model  = M_BERT\n",
    "            masked = \"[MASK]\" + words[-n][-1] if not words[-n][-1].isalpha() else \"[MASK]\"\n",
    "            target = words[-n].strip(punctuation)\n",
    "            prompt = ' '.join(words[:-n] + [masked] + words[len(words)-(n-1):])\n",
    "        else:\n",
    "            model  = M_GPT2\n",
    "            string = string.strip()\n",
    "            last_space = string.rfind(' ')\n",
    "            prompt = string[:last_space]\n",
    "        probs.append(model.get_word_probs(prompt))\n",
    "    return probs\n",
    "def correction(string, back_n, pprobs=False):\n",
    "    places = reversed(range(1,back_n+1))\n",
    "    if back_n == 0: places = [1, 3, 2]\n",
    "    string = string.strip()\n",
    "    words  = string.split()\n",
    "    last_space = string.rfind(' ')\n",
    "    for n in places:\n",
    "        if n > len(words) or len(words) == 1: break\n",
    "        spelled = False\n",
    "        if n > 1:\n",
    "            model  = M_BERT\n",
    "            masked = \"[MASK]\" + words[-n][-1] if not words[-n][-1].isalpha() else \"[MASK]\"\n",
    "            target = words[-n].strip(punctuation)\n",
    "            prompt = ' '.join(words[:-n] + [masked] + words[len(words)-(n-1):])\n",
    "            target = words[-n].strip(punctuation)\n",
    "            if target != spell(target):\n",
    "                spelled = True\n",
    "        else:\n",
    "            model  = M_GPT2\n",
    "            string = string.strip()\n",
    "            last_space = string.rfind(' ')\n",
    "            prompt = string[:last_space]\n",
    "            target = string[last_space+1:].strip(punctuation)\n",
    "            target = words[-n].strip(punctuation)\n",
    "            if target != spell(target):\n",
    "                spelled = True\n",
    "        if pprobs:\n",
    "            probs = pprobs[n-1]\n",
    "        else:\n",
    "            probs = model.get_word_probs(prompt)\n",
    "        probs[:,1] = probs[:,1].astype(float)/probs[:,1].astype(float).sum()\n",
    "        probsp = [(str(word), float(prob), float(similar(target.lower(), word.lower()))) for word, prob in probs if word in wl]\n",
    "        close_probs = [prob for prob in probsp if prob[2] > 0.5 and prob[1] >= min(0.001, probsp[consider_top][1])]\n",
    "        props = [(word, (prob**prob_exp)*log_map(log_exp)(sim)) for word, prob, sim in close_probs]\n",
    "        props = sorted(props, reverse=True, key=lambda x: x[1])\n",
    "        props = [prop for prop in props if prop[1] > 0.000001]\n",
    "        probN = threshold(n)\n",
    "        make_correction = False\n",
    "        if len(props) > 0 and props[0][1] > probN:\n",
    "            make_correction = True\n",
    "            irr_t = props[0][1] * relevency_t\n",
    "            for word, score in props: \n",
    "                if score < irr_t: break\n",
    "                elif target.lower() == word.lower() or stem(target.lower())  == word.lower() or lemma(target.lower()) == word.lower():\n",
    "                    make_correction = False\n",
    "        if make_correction: return (n, props[0][0], spelled)\n",
    "        #if spelled: return (n, target, spelled)\n",
    "    return False\n",
    "def process_correction(string, back_n, pprobs=False):\n",
    "    corrected = string\n",
    "    words     = string.split()\n",
    "    is_correction = correction(string, back_n, pprobs)\n",
    "    if is_correction:\n",
    "        n, word, _ = is_correction\n",
    "        words[-n] = word if words[-n][-1] not in punctuation else word + words[-n][-1]\n",
    "        corrected = \" \".join(words)\n",
    "        #is_correction = correction(corrected, back_n)\n",
    "    return corrected if corrected != string else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c45343-68e5-47fc-b767-18e4f285ef9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['when you come in can you remember to feel the cat?',\n",
       "        'when you come in can you remember to feed the cat?'],\n",
       "       ['when you come in can you remember to feed the cad?',\n",
       "        'when you come in can you remember to feed the cat?'],\n",
       "       ['when you come in can you remember to feed the cad? He',\n",
       "        'when you come in can you remember to feed the cat? He'],\n",
       "       ['when you come in can you remember to feed the cad? He needs',\n",
       "        'when you come in can you remember to feed the cat? He needs'],\n",
       "       ['can you remember to feed the cad when',\n",
       "        'can you remember to feed the cat when']], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = pd.read_csv(\"strings.txt\", quotechar='\"', header=None, index_col=False, skipinitialspace=True).values\n",
    "strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933b304f-f254-41a7-b380-6a282482fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [get_probs(string[0], 4) for string in strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3f5aa2-3d7c-42b5-be74-e297bf89b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_n  = 4  # number of words back from end of string, 1 is just last word, 0 is [1, 3, 2]\n",
    "k       = 1.2  # exponent parameter for exponential decay of word length augmedented SequenceMatcher\n",
    "ap      = 0.55  # exponent parameter\n",
    "bp      = 1  # exponent parameter\n",
    "log_exp      = 5  # exponent parameter for logarithmic mapping\n",
    "prob_exp     = 1  # raise probability to power in ((prob**power)*log-sim)\n",
    "consider_top = 100  # max top model word predictions considered\n",
    "relevency_t  = 0.07  # threshold defined by portion of top proposition to exclude much smaller scored propositions for correcting\n",
    "base_t       = 0.0015   # decision threshold for last word: base threshold\n",
    "threshold_e  = 1.8  # exponent for exponential thresholds\n",
    "threshold_t  = \"exponential\"  # function defines decision threshold for word n from end\n",
    "threshold    = {\"constant\":    lambda n: base_t,\n",
    "                \"linear\":      lambda n: base_t + (base_t * (n-1)),\n",
    "                \"exponential\": lambda n: base_t * (n**threshold_e),\n",
    "                \"jump-exp\":    lambda n: base_t * (max(n-1,1)**threshold_e),        # jump thresholds start growing after n=2\n",
    "                \"jump-lin\":    lambda n: base_t + (base_t * max(n-2, 0))\n",
    "               }[threshold_t]\n",
    "\n",
    "corrections = []\n",
    "for pprobs, (x, _) in zip(probs, strings): \n",
    "    corrections.append(process_correction(x, back_n, pprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2e4912-7822-48a2-b9e8-7a7ec155243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TP=71, TTP=67, TN=101, FN=48, FP=2, FTP=4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_h = np.array(corrections == strings[:,1])\n",
    "same = strings[:,0] == strings[:,1]\n",
    "not_corrected = np.array(corrections) == \"False\"\n",
    "tn  = np.logical_and(same, not_corrected)\n",
    "fn  = np.logical_and(np.logical_not(same), not_corrected)\n",
    "fp  = np.logical_and(same, np.logical_not(not_corrected))\n",
    "tp = np.logical_and(np.logical_not(same), np.logical_not(not_corrected))\n",
    "ttp  = np.logical_and(tp, y_h)\n",
    "TN = tn.sum(); FN = fn.sum(); FP = fp.sum(); TP = tp.sum(); TTP = ttp.sum(); TP = tp.sum(); FTP = TP-TTP\n",
    "# [Total] True Positives, True True Positives, True Negatives, False Negatives, False Positives, False True Positives\n",
    "f'TP={TP}, TTP={TTP}, TN={TN}, FN={FN}, FP={FP}, FTP={FTP}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccfddb6-7c44-4bc5-a35d-96d576e32c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'precision=0.918, recall=0.597, specificity=0.944, accuracy=0.757, f1=0.723'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision   = TTP/(TP+FP)\n",
    "recall      = TP/(TP+FN)\n",
    "specificity = TN/(TN+FP+FTP)\n",
    "accuracy    = (TTP+TN)/(TP+TN+FP+FN)\n",
    "f1          = (2*precision*recall)/(precision+recall)\n",
    "f'precision={precision:.3f}, recall={recall:.3f}, specificity={specificity:.3f}, accuracy={accuracy:.3f}, f1={f1:.3f}'#, f'k={k}, a={ap}, b={bp}, log_exp={log_exp}, prob_exp={prob_exp}, relevency_t={relevency_t}, base_t={base_t}, threshold_e={threshold_e}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
